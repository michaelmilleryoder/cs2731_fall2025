[
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Project",
    "section": "",
    "text": "Last revised 2025-11-04.\nA major component of this course is a hands-on final project guided by students’ own interests. In this project, students will demonstrate an ability to summarize current approaches and challenges in a subfield of NLP and implement some sort of contribution (however small) to this NLP area of research or practice.\nProjects will be done in groups of 2-4 students. Groups will be formed during an in-class project match day, largely based on interest in the same project ideas."
  },
  {
    "objectID": "project.html#project-idea-form",
    "href": "project.html#project-idea-form",
    "title": "Project",
    "section": "Project idea form",
    "text": "Project idea form\nDue 09-11.\nFill out project ideas you might be interested in working on in this form. You can fill out ideas from the example projects listed below or one of your own ideas. For your own ideas, consider what what research you’re interested in, what system you’d like to build that processes language in some form, interesting text datasets you’d like to work on, really anything! It is best if your idea has a dataset in mind, but this is not required.\nYou can fill out as many ideas as you’d like with this form. Ideas do not have to be fully sketched out. Submitting an idea does not mean you will necessarily work on it. These ideas will be presented to all students anonymously. Each student must submit at least one idea for credit on this assignment, even if it’s just chosen from the example projects.\n\nExample projects\nSome of these projects are drawn from “shared tasks” where NLP researchers compete for the best performance on certain datasets. Others are based on ideas and projects from prior students and from the instructor.\n\n1. Text classification\n\nClassify adversarial prompts for LLMs based on attack type, using publicly available red-teaming datasets.\nClassify discourse relations, the relationship between pieces of text within longer narratives (such as the reason or supporting evidence behind a claim). See DISRPT 2025 shared task 3, data repo here in collaboration with Prof. Janet Liu in the Linguistics Department.\nGiven a review of a restaurant, determine what type of restaurant it is from this Yelp dataset\nGiven a short essay in response to a troubling news article, predict the level of empathy. See WASSA 2024 shared task Track 3.\nPredict emotion labels from tweets across many languages. See WASSA 2024 shared task\nGiven a news article and a list of “entities” (people, organizations, etc), predict roles such as protagonist, antagonist, and innocent. See SemEval 2025 Task 10, Subtask 1 on entity framing\nPredict news genre or media “frames” such as morality, economic, or crime and punishment from news articles in multiple languages. See SemEval 2023 Task 3, Subtasks 1 or 2\nPredict whether text was written by humans or generated by AI. Tasks include predicting for data across languages and for academic essays. See GenAI Content Detection Workshop, Task 1 or 2\nClassify tweets as sexist or not, or predict the “intent” of sexist tweets as direct, reported or judgemental. See EXIST 2024 Task 1 or Task 2\nPredict if similar words are redundant or not with the Semantic Pleonasm corpus developed right here at Pitt.\nFrom a set of descriptions of characters, develop a classifier to predict which ones will generate the most fanfiction. This could be a lens into online community and media norms.\nPredict “speech acts”, intentions behind utterances, based on emojis with a dataset assembled by former students in the class.\n\n\n\n2. Machine translation\n\nTrain translation models for literary text and evaluate on a dataset of Korean-English webnovels developed by a former student in this class.\nTranslate customer service chats in between languages. See the WMT 2024 Chat Shared Task\nTranslate code-mixed Hinglish to English. See the WMT 2022 Code-mixed Machine Translation Task\nCreate a system to automatically correct (post-edit) machine translations. See the WMT 2022 Automatic Post-Editing Shared Task\n\n\n\n3. Information retrieval and extraction\n\nGiven a query, retrieve the most relevant passages from regulatory documents: https://www.codabench.org/competitions/3527/\nExtract important entities from scientific articles with the SCIRex dataset\n\n\n\n\n4. Analysis and annotation of datasets\n\nImprove part-of-speech tagging and other linguistic annotation for spontaneous speech in the Archive of Pittsburgh Language and Speech (APLS) with collaborator Prof. Dan Villarreal in the Linguistics Department. An evaluation dataset for parts of speech has already been manually annotated and so this project is ready to go to evaluate different systems! This work would help linguistics researchers study specific linguistic phenomena by speakers here in Pittsburgh.\nVisualize similarities in US state legislature bill texts and predict bill passage using data from LegiScan (example repo here).\nDevelop an annotation guide and start annotating a new dataset of online gaming voice chat for hate speech, abusive, and offensive language.\nHate speech is culturally specific, yet the majority of NLP work focuses on English in North American and European contexts. A quantitative analysis of different features of datasets annotated for hate speech in multiple languages and from multiple cultural contexts would illuminate global similarities and culturally specific contexts.\nFanfiction, online writing by fans of media works, is known for celebrating queer identity but still may center the experiences of white authors and characters. Use FanfictionNLP to compare representations of characters of color to white characters in fanfiction at scale.\nQuantitative analysis of hateful, white supremacist narratives usually centers on contemporary online discourse. Yet many white supremacist language and narratives has its roots before online discourse. Compare narratives, topics and themes presented in historic and contemporary white supremacist discourse with data provided by the instructor.\nExplore similarities and differences between language in podcasts and Reddit communities based on those podcasts using a dataset assembled by former students in the class.\nComputational analysis of Palestinian Nakba narratives. See workshop and datasets.\nExamine the framing of different entities in police Facebook posts from the Plain View Project.\nAnalyze how different newspapers cover topics differently in English-language editorials from Sri Lankan newspapers. Data is provided by the instructor and a collaborator at Carnegie Mellon University.\n\n\n\n5. Survey papers\n\nSurvey how NLP is used and applied in other fields before and after LLMs. What has been our most useful contributions to scholars in the social sciences, physical sciences, or humanities? This survey would assemble papers across disciplines for mentions of NLP and summarize what is most useful, what is lacking, and what approaches from NLP could be helpful to others.\nComputational social science using NLP generally relies on data from online communities. But this is missing non-online interactions and the practices of those who are not active online. Survey datasets and approaches that use quantitative and computational techniques on recordings of offline linguistic interaction.\nA growing area of research in computational social science aims to capture the framing and portrayal of entities across large text corpora (such as in news media). Survey existing approaches and challenges.\n\n\n\n6. Other\n\nEvaluate LLMs for their factuality in summarization of class reflections using a dataset provided by the instructor and Prof. Diane Litman.\nEvaluate the fairness of quality scores automatically assigned to sutdent reflections using a dataset provided by the instructor and Prof. Diane Litman.\nNew identity terms are commonly developed in online communities, some of them hateful. Develop methods to find in-group hate jargon and identity terms.\nBuild networks of characters and predict relations among characters in fiction using this dataset.\nStancetaking, a concept from sociolinguistics, is when speakers take an evaluative position toward the concept (which are often nuanced, e.g. “No, I actually don’t like Taylor Swift’s music that much, but she’s great as a person”). Develop automated methods for identifying the “stance object”, who the speaker is evaluating, likely from Reddit data.\nAutomatically summarize movies based on their subtitles from this dataset developed by former students in the class."
  },
  {
    "objectID": "project.html#project-group-match-day",
    "href": "project.html#project-group-match-day",
    "title": "Project",
    "section": "Project group match day",
    "text": "Project group match day\nIn class 09-17.\nStudents will form groups of 2-4 people around a list of potential projects submitted by the class in the project idea form. Note that this list of project ideas is much greater than the final number of groups will be, so not all project ideas will have groups.\nProject idea list\n\nClassify adversarial prompts for LLMs based on attack type, using publicly available red-teaming datasets. This project is the same as project 1.1 in the example list.\nGiven a review of a restaurant, determine what type of restaurant it is from this Yelp dataset. This project is the same as project 1.3 in the example list.\nImprove part-of-speech tagging and other linguistic annotation for spontaneous speech in the Archive of Pittsburgh Language and Speech (APLS) with collaborator Prof. Dan Villarreal in the Linguistics Department. An evaluation dataset for parts of speech has already been manually annotated and so this project is ready to go to evaluate different systems! This work would help linguistics researchers study specific linguistic phenomena by speakers here in Pittsburgh. This project is the same as project 4.1 in the example list.\nClassify discourse relations, the relationship between pieces of text within longer narratives (such as the reason or supporting evidence behind a claim). See DISRPT 2025 shared task 3, data repo here in collaboration with Prof. Janet Liu in the Linguistics Department. This project is the same as project 1.2 in the example list.\nGiven a short essay in response to a troubling news article, predict the level of empathy. See WASSA 2024 shared task Track 3. This project is the same as project 1.4 in the example list.\nPredict whether text was written by humans or generated by AI. Tasks include predicting for data across languages and for academic essays. See GenAI Content Detection Workshop, Task 1 or 2 and dataset. This project is the same as project 1.8 in the example projects list above.\nPredict emotion labels from tweets across many languages. See WASSA 2024 shared task. This project is the same as project 1.5 in the example projects list above.\nPredict “speech acts”, intentions behind utterances, based on emojis with a dataset assembled by former students in the class. This project is the same as project 1.12 in the example projects list above.\nBuild networks of characters and predict relations among characters in fiction using this dataset. This project is the same as project 6.4 in the example projects list above.\nLink extracted clinical entity with standardized clinical coding with the SNOMED CT challenge.\nExtract and organize crowd-sourced media tropes from TV Tropes and analyze their sources or identify them in media texts, other fandom wikis, or fanfiction.\nEvaluate LLMs for their factuality in summarization of class reflections, or the fairness of summarization scores, using a dataset provided by the instructor and Prof. Diane Litman. This project is the same as project 6.4 in the example projects list above.\nExtracting claims from social media posts for the intent of claim fact-checking.\nBuild a Pokémon chatbot or classifier for Pokémon based on text descriptions from Bulbapedia and entries from Pokédex.\nBuild a speech-to-speech translation system between Indian languages with data from Project Vaani.\nTrain an LLM to play a Text Adventure Game (https://writtenrealms.com/). Reinforcement learning or an LLM agent with a few examples (few-shot prompting) are some example approaches to play the game."
  },
  {
    "objectID": "project.html#project-peer-group-feedback",
    "href": "project.html#project-peer-group-feedback",
    "title": "Project",
    "section": "Project peer group feedback",
    "text": "Project peer group feedback\nIn class 10-15.\nIn class before the proposal is due, you will be matched with another group who will review your proposal and provide guided feedback in class."
  },
  {
    "objectID": "project.html#project-proposal",
    "href": "project.html#project-proposal",
    "title": "Project",
    "section": "Project proposal",
    "text": "Project proposal\nDue 10-17.\nPlease submit one per group on Canvas. There is no required length or format for this report, but it is recommended to use the ACL format that the final report will be formatted in. This proposal will contain answers to a series of questions. It will include a peer review where you will rate your own performance and the performance of other group members through the form here.\n\nTask: What is the problem or task you are focusing on?\nInput and output: What is the format of the input and output of this task? For example, each input could be a sentence of text and the output could be a label from a discrete set of possible labels. Provide at least one example of input and output from your data (ideally actual input and output, but it’s fine if they are made up).\nLiterature review: How does your contribution build on or extend prior work? How have others approached your task or similar tasks? What are other NLP papers that use the same dataset or domain as your project? This literature review will be of at least 3 papers relevant to your project area. It will group and summarize relevant papers into types of tasks, datasets, and/or approaches. Good places to look for NLP papers include the ACL Anthology, Semantic Scholar, and Google Scholar.\nData: What data are you using? How many datapoints does the dataset contain and what is the composition of each datapoint? Please explain where these datasets are from and how they were constructed. Has any other research been published using these datasets? Provide links to any URLs if the data is hosted online or cite papers if the dataset is published somewhere. Does the dataset have annotated labels or “gold” text that you are predicting or generating provided? If so, where do those labels come from?\nMethods: What approaches are you planning on taking to address the task? What models and if appropriate, what methods of extracting features from text will be used? Prompting strategies for decoder-only LLMs are good, but there should be some comparison with another strategy, such as fine-tuning an encoder-only LLM such as BERT or comparing with classical statistical and n-gram approaches. Talk to the instructor if you have more questions about this, as there is flexibility depending on your specific project.\nEvaluation: How are you evaluating your approach? What performance metrics are you going to use?\nEthics: What kinds of ethical issues may be raised by your model or data?\nSteps: What are the proposed steps needed for completion of (your proposed part) of the project? This should be in some detail, for example, loading and potentially cleaning the data, training models, trying different parameters, evaluating models, etc.\nRoles: What are roles and tasks of each person in the group? Though group members will contribute in various capacities, it is best if each person is responsible for at least one aspect of the project."
  },
  {
    "objectID": "project.html#project-proposal-presentation",
    "href": "project.html#project-proposal-presentation",
    "title": "Project",
    "section": "Project proposal presentation",
    "text": "Project proposal presentation\nIn class 10-20.\nGroups will make a brief presentation to the class outlining their proposed project, with Q&A and opportunities for feedback from other students. Please plan for maximum 7-minute presentations not including Q&A, which will be held right afterward for each group. Slides will be added to this shared PowerPoint presentation. Presentations are not graded. Cover at least these key points:\n\nProject motivation\nBriefly, what 1-2 other related papers have done (1 slide max)\nWhat data you are planning to use\nWhat approach/methods you plan to take\nHow you will evaluate your approach"
  },
  {
    "objectID": "project.html#progress-report",
    "href": "project.html#progress-report",
    "title": "Project",
    "section": "Progress report",
    "text": "Progress report\nDue 11-13.\nA brief progress report of a basic working system. This report should be in the ACL format that the final report will be in.\n\nPart 1: Task and dataset\nIn this part, please provide the following information about your dataset. It’s fine to be working with multiple datasets; just complete this for each one or for a final dataset you will be using if you are combining datasets.\n\nWhat is the problem/task you are working on? What is the input and output? Just a brief refresher is fine.\nHow will the dataset you have (or that you will build) be used to implement a system to address this task?\nThe number of rows (datapoints) in the dataset and what each datapoint corresponds to. If you are splitting the dataset into a training, test, and possible dev sets, how many rows are in each?\nThe number of columns in the dataset you will be using and what each corresponds to.\nIf applicable, the distribution of the target labels you are predicting. So for a binary sentiment classification task, how many rows in each set (except the test set) are marked negative or positive sentiment? This can be in a table or graph format.\nPlease explain any decisions made in selecting or preprocessing data.\nOptionally, any other distribution or data visualization that you think is helpful for understanding your dataset or task. An example table with a small part of the data can be helpful.\n\n\n\nPart 2: Some kind of result\nPlease provide one (hopefully quantitative) result from your work so far. A good example would be a performance metric result from your baseline approach on a dev or test set, but it could also be some sort of other finding you have so far. But if you’re not that far yet, you can also provide an example of working input and output from your system or part of a system, some sort of plot or other output. You can be up front about challenges you are facing for which you might need help. To get a good grade, I’ll just be looking for some sort of output from a working system or part of a system. If you are confused what this means for your project, contact the instructor.\n\n\nPart 3: Open questions and challenges\nPlease describe any open questions or challenges your group has at this point. Will you need any resources other than the ones provided in class (LLM APIs, CRCD access) or have any other questions? Also describe if the roles for each of your team members have changed since the proposal and if so, what the new roles are."
  },
  {
    "objectID": "project.html#final-presentation",
    "href": "project.html#final-presentation",
    "title": "Project",
    "section": "Final presentation",
    "text": "Final presentation\nIn class 12-08.\nGroups will present their finished work to the group, with Q&A and feedback opportunities from students. Please prepare a maximum 7-minute presentation.  Cover at least these key points:\n\nProject motivation (briefly)\nTask description, including example input and output\nData\nMethods\nResults or findings"
  },
  {
    "objectID": "project.html#final-report",
    "href": "project.html#final-report",
    "title": "Project",
    "section": "Final report",
    "text": "Final report\nDue 12-09.\nAt the end of the course, groups will provide a written report of their project. This project includes a quantitative comparison between at least two NLP systems on a clearly specified task or tasks. One of these is generally a more traditional NLP approach and the other involves LLMs, though your group’s project may vary if you have discussed this with the instructor.\nAt the end of the course, groups will provide a written report of their project. This report will be in the ACL format found here (Overleaf template here). The report should be a maximum of 8 pages, not including limitations, ethics, group member task breakdown, references sections or appendices. Outstanding reports would be of a quality and structure that could be submitted to an NLP workshop or conference, but other types of projects can also achieve an A. There is flexibility in section names, but please provide information about the following aspects of the project:\n\nProject motivation\nLiterature review. Please provide full citations in a references sections for works cited throughout the paper (not just URLs).\nData\nMethods. Please clearly specify which techniques are novel/your own versus methods directly or indirectly from prior work (which is also fine).\nResults\nDiscussion\nFuture work. This is a good place to describe things you thought about but never had time to complete!\nLimitations (doesn’t count toward page limit)\nEthical issues (doesn’t count toward page limit)\nGroup member task breakdown (doesn’t count toward page limit). This section details the high-level tasks that each group member completed.\nReferences (doesn’t count toward page limit)\nAppendices (optional, doesn’t count toward page limit). Additional figures or explanation in one or more appendices is allowed, but they will not necessarily be considered in grading."
  },
  {
    "objectID": "project.html#how-your-project-will-be-graded",
    "href": "project.html#how-your-project-will-be-graded",
    "title": "Project",
    "section": "How your project will be graded",
    "text": "How your project will be graded\nTo get an A, your group’s project should make progress toward an achievable, concrete contribution specified in your project proposal. The project does not necessarily need to be successful in the sense that it outperforms baselines or contributes to our knowledge of a phenomenon. Sometimes ideas don’t work, and that’s okay. But you need to provide evidence of progress toward that contribution. If you are building a dataset, for example, the dataset needs to be built in some form, even if it is as not as large or as useful as you may have hoped. If you are evaluating a new method for a task, you must have an implementation that tests that method against other baselines, even if it doesn’t perform as well as you would have hoped or you didn’t get to evaluate it against all the baselines you wanted to. If you are doing a survey, you must distill a sufficient number of papers into themes that comprehensively describe a research area, even if you don’t end up finding groundbreaking gaps in knowledge that must be addressed. Feel free to take on more risky ideas, but only if you know you’ll have something to show for it at the end. Teaching staff will guide you toward scoping projects that should fulfill this goal in the planning phase through the proposal."
  },
  {
    "objectID": "hw4.html",
    "href": "hw4.html",
    "title": "Homework 4: Sequence labeling",
    "section": "",
    "text": "Due 2025-11-20, 11:59pm. Instructions last updated 2025-11-06."
  },
  {
    "objectID": "hw4.html#deliverables",
    "href": "hw4.html#deliverables",
    "title": "Homework 4: Sequence labeling",
    "section": "Deliverables",
    "text": "Deliverables\nIn your report, include: 1. The 5 most frequent POS tags for English and Norwegian datasets (specified in the notebook) and how many tokens are tagged with each 1. For each of the 5 most frequent POS tags for English and Norwegian datasets, provide the 5 most frequent word types annotated with that tag in the training data 1. The names of the pretrained BERT-based models you chose for both English and Norwegian 1. A brief discussion of any choices you made about hyperparameters in training 1. (Optionally, for extra credit) A description of changes you made or different pretrained models you tried and what accuracy you obtained on the dev set. 2 point of extra credit (total) will be given if any changes result in an improved accuracy on the dev set. 1. Accuracy of the fine-tuned models on the test set for both English and Norwegian 1. POS tags predicted for the words of a sentence of your choice in both English and Norwegian 4. A link to your copied and filled out Colab notebook. Please set the Colab document sharing to public viewing so that teaching staff can see it for grading."
  },
  {
    "objectID": "hw4.html#submission",
    "href": "hw4.html#submission",
    "title": "Homework 4: Sequence labeling",
    "section": "Submission",
    "text": "Submission\nPlease submit the following items on Canvas:\n\nYour code: the Jupyter notebook you modified from the template. Submit:\n\nyour .ipynb file\na .html export of your notebook. To get a .html version, click File &gt; Save and Export Notebook As… &gt; HTML from within JupyterLab.\n\nYour report with deliverables named report_{your pitt email id}_hw4.pdf. No need to include @pitt.edu, just use the email ID before that part. For example: report_mmyoder_hw4.pdf.\nA README.txt file explaining\n\nany additional resources, references, or web pages you’ve consulted\nany person with whom you’ve discussed the assignment and describe the nature of your discussions\nany generative AI tool used, and how it was used\nany unresolved issues or problems"
  },
  {
    "objectID": "hw4.html#acknowledgments",
    "href": "hw4.html#acknowledgments",
    "title": "Homework 4: Sequence labeling",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis assignment is adapted from Jacob Eisenstein and Prof. Yulia Tsvetkov."
  },
  {
    "objectID": "hw2.html",
    "href": "hw2.html",
    "title": "Homework 2: Text classification",
    "section": "",
    "text": "Due 2025-10-09, 11:59pm. Instructions last updated 2025-10-07."
  },
  {
    "objectID": "hw2.html#learning-objectives",
    "href": "hw2.html#learning-objectives",
    "title": "Homework 2: Text classification",
    "section": "Learning objectives",
    "text": "Learning objectives\nAfter completing this assignment, students will be able to:\n\nImplement a text classification system using logistic regression and feature-based approaches\nEvaluate a text classification system\nIdentify informative features in a feature-based text classification system\nAnalyze errors in an NLP system"
  },
  {
    "objectID": "hw2.html#implement-a-deception-classifier",
    "href": "hw2.html#implement-a-deception-classifier",
    "title": "Homework 2: Text classification",
    "section": "Implement a deception classifier",
    "text": "Implement a deception classifier\nYou will design and implement a program to classify if a comment from a player of the Diplomacy game is truthful or not.\nYou can use any packages you want for this (scikit-learn, spaCy, NLTK, Gensim, etc., as well as code from in-class example notebooks). Any packages used, along with version numbers, should be specified in a requirements.txt file. The version of Python used should also be specified in your README.txt file. If you will be using a language other than Python, please let us know before submitting."
  },
  {
    "objectID": "hw2.html#dataset",
    "href": "hw2.html#dataset",
    "title": "Homework 2: Text classification",
    "section": "Dataset",
    "text": "Dataset\nHere is the dataset that you should download for this assignment:\n\ndiplomacy_train.csv. This dataset has a variety of fields, but the most important are:\n\ntext: the text of the comment\nintent: 0 for truth, 1 for lie\n\ndiplomacy_dev.csv. This is the development set to be used for evaluation and error analysis.\ndiplomacy_kaggle.csv. This data has the same fields as the training data, but does not have the “correct” intent filled in. This file is to be used as a test set for the challenge competition hosted on Kaggle.\n\nThe data is from a recording of online players of Diplomacy, as presented in Peskov et al. 2020. Negotiation and back-stabbing are key elements of the Diplomacy game."
  },
  {
    "objectID": "hw2.html#part-1-feature-based-logistic-regression-models",
    "href": "hw2.html#part-1-feature-based-logistic-regression-models",
    "title": "Homework 2: Text classification",
    "section": "Part 1: Feature-based logistic regression models",
    "text": "Part 1: Feature-based logistic regression models\nIn this section, you will build a logistic regression model based on bag-of-word features and/or features of your own design, trained on diplomacy_train.csv. You can do whatever preprocessing you see fit. You will report performance on the diplomacy_dev.csv dataset.\nImplement and try the following feature and model combinations:\n\nLogistic regression with bag-of-words (unigram) features. Build a logistic regression classifier that uses bag-of-words (unigram) features.\nLogistic regression with your own features/change in preprocessing. Design and test at least two modifications (custom features or preprocessing changes) to unweighted unigram features. Note that these features can be used in conjunction with bag-of-words features or by themselves. Possible features/changes to add and test include:\n\nTf-idf transformed bag-of-words features. See J+M section 11.1.1 for a description of tf-idf\nHigher order n-gram features (bigrams, trigrams, or combinations of them) beyond the unigrams used for the bag-of-words features\nDifferent preprocessing (stemming, different tokenizations, stopword removal)\nChanging count bag-of-words features to binary 0 or 1 for the presence of unigrams\nIncorporating features from columns in the dataset other than text\nReducing noisy features with feature selection\nCounts or added weight from custom word lists\nAny other custom-designed feature (such as length of input, number of capitalized words, neural embeddings of text, etc)\n\n\nYou will thus have 3 total logistic regression models: one using bag-of-word features and 2 with your own selected features or preprocessing changes.\n\nInclude in the report\n\n1. Performance table\nReport a table of performance scores for models trained on each set of features, evaluated on diplomacy_dev.csv. Include accuracy as well as the following metrics for the positive (lying) class: precision, recall, and f1-score.\n\n\n2. Feature descriptions\nFor each feature or change in input text processing:\n\nDescribe your motivation for including the feature\nDiscussion of results: Did it improve performance or not? (Either result is fine. It is not necessary to beat logistic regression with unigram features.)\n\n\n\n3. Informative features and error analysis\nFor a feature-based model of your choice:\n\nExtract and discuss the most informative features that are mostly strongly positively and negatively associated with deception. Please normalize feature values to some sort of standard scale for interpretation. Report the 5 features with the highest weights and 5 features with the lowest (negative) weights. Discuss how these may or may not make sense for this task. You may adapt code provided by the instructor, use another source online, or write your own. Give specific informative features, such as particular words (e.g. “actually”) for bag-of-words features, instead of sets of features like “bigram features”.\nDo an error analysis. On the dev set, provide a confusion matrix. Sample examples from both false negatives and false positives and present a few of them in the report. Do you see any patterns in these errors? How might these errors be addressed with different features or if the system could understand something else? (You don’t have to implement these, just speculate.)"
  },
  {
    "objectID": "hw2.html#part-2-submit-your-classifier-in-the-class-challenge",
    "href": "hw2.html#part-2-submit-your-classifier-in-the-class-challenge",
    "title": "Homework 2: Text classification",
    "section": "Part 2: Submit your classifier in the class challenge",
    "text": "Part 2: Submit your classifier in the class challenge\nPlease submit your classifier to run on a hidden held-out test set as part of a class competition. It is necessary to submit one of your classifiers, but you will not be graded on your performance. Instead, bonus points will be awarded for top systems. Bonus points will be awarded in the competition as follows, as measured by accuracy on our held-out test set.\n\n6 bonus points for the best-performing logistic regression classifier\n4 bonus points for the 2nd best-performing logistic regression classifier\n2 bonus points for the 3rd best-performing logistic regression classifier\n\n\nHow to submit your classifier\nThis optional competition is conducted on Kaggle. See this page for instructions on how to submit: https://www.kaggle.com/t/77844262480c47fe86f75dc4c3a13848\nYou will need to create a Kaggle account to submit. Let the instructor know if this is a barrier and we will work something out. Please provide your Kaggle username used in the competition in your report. Note that this username will be visible in a leaderboard to other challenge competition participants."
  },
  {
    "objectID": "hw2.html#notes",
    "href": "hw2.html#notes",
    "title": "Homework 2: Text classification",
    "section": "Notes",
    "text": "Notes\n\nDon’t feel like you need to write things from scratch; use as many packages as you want. Class Jupyter notebooks, Google, Stack Overflow, and NLP/ML software documentation are your friend! Adapting and consulting other approaches is fine and should be noted in comments in the code and/or in the README.txt. Just don’t use complete, fully-formed implementations, including from generative AI tools. Use all resources as aids, not as a final product.\nOptionally, you may incorporate any form of regularization that you like."
  },
  {
    "objectID": "hw2.html#deliverables",
    "href": "hw2.html#deliverables",
    "title": "Homework 2: Text classification",
    "section": "Deliverables",
    "text": "Deliverables\n\nYour report with results and answers to questions in Part 1, named hw2_{your pitt email id}.pdf. No need to include @pitt.edu, just use the email ID before that part. For example: report_mmyoder_hw2.pdf.\nYour code used to train models and estimate performance for Part 1 in a file named hw2_{your pitt email id}_train.py.\nYour code used for the Kaggle submission in Part 2 in a file named hw2_{your pitt email id}_kaggle.py.\nA README.txt file explaining\n\nThe Kaggle username you used to submit your predictions\nhow to run the code you used to train your models and calculate dev set performance\nthe version of Python used\nany additional files needed to run the code\nany additional resources, references, or web pages you’ve consulted\nany person with whom you’ve discussed the assignment and describe the nature of your discussions\nany generative AI tool used, and how it was used\nany unresolved issues or problems\n\nA requirements.txt file with:\n\nall Python packages and package versions in the computing environment you used in case we replicate your experiments\n\n\nPlease submit all of this material on Canvas. We will grade your report and look over your code."
  },
  {
    "objectID": "hw2.html#grading",
    "href": "hw2.html#grading",
    "title": "Homework 2: Text classification",
    "section": "Grading",
    "text": "Grading\nSee rubric on Canvas."
  },
  {
    "objectID": "hw2.html#acknowledgments",
    "href": "hw2.html#acknowledgments",
    "title": "Homework 2: Text classification",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis assignment is inspired from a homework assignment by Prof. Diane Litman. Data is from Peskov et al. 2020."
  },
  {
    "objectID": "hw0.html",
    "href": "hw0.html",
    "title": "Homework 0: CRCD JupyterHub setup",
    "section": "",
    "text": "Due 2025-09-05, 11:59pm. Instructions last updated 2025-09-01.\nThis assignment is simply for you to get set up with the running Jupyter notebooks on the CRCD JupyterHub. We will use JupyterHub for in-class coding activities. Please follow the instructions exactly as listed. I’m interested in seeing if students can successfully load JupyterHub with the custom class conda environment; please don’t submit screenshots using pip to install packages in the base environment."
  },
  {
    "objectID": "hw0.html#instructions",
    "href": "hw0.html#instructions",
    "title": "Homework 0: CRCD JupyterHub setup",
    "section": "Instructions",
    "text": "Instructions\n\nLog in to the CRCD JupyterHub at jupyter.crc.pitt.edu with your Pitt credentials. Type your username in lowercase and make sure that you are on the WIRELESS-PITTNET wifi network. If you are off-campus or not on WIRELESS-PITTNET, you will need to log in to the Pitt VPN through the GlobalProtect app. Instructions for setting that up are here. Reach out to Michael if you are not able to log in.\nSelect the following JupyterHub Session Configuration options:\n\nSelect Partition: TEACH - 6 CPUs - 45GB\nSelect Virtual Environment: Provide custom path\nCustom Environment Path: /ix/cs2731_2025f/class_env. Note that this is different from what we did in class.\nAccount: you can leave this blank If you can’t spawn a server at this point, try setting Account to cs2731_2025f. Also make sure you have enough space in your CRCD home directory. Check your usage by using ssh to log in to h2p.crc.pitt.edu and running crc-quota.\n\nClick the following nbgitpuller link. This should load the latest version of session2_preprocessing.ipynb, which starts with a section header of Import necessary packages and should look like this. If it starts with Install nltk or anything else, close the notebook and right-click the notebook filename on the side panel to delete it. Click the nbgitpuller link again to load the latest version.\nDouble-click session2_preprocessing.ipynb on the left-hand side panel to open the Jupyter notebook.\nRun the cells under the Import necessary packages section. That is all you have to do. You do not have to run the rest of the notebook.\nTake a screenshot of the notebook (only the output of the first section is necessary) and submit it on Canvas. Ideally you are able to import pandas and nltk, but even if there are errors, that’s okay! You will still get credit for the assignment, and I will message you on Canvas to try to figure any issues out."
  },
  {
    "objectID": "hw1.html",
    "href": "hw1.html",
    "title": "Homework 1: Character-level language modeling",
    "section": "",
    "text": "Due 2025-09-26, 11:59pm. Instructions last updated 2025-09-23.\nLanguage modeling is the task of predicting the next word in a sequence given the previous words. In this assignment, we will focus on the related problem of predicting the next character in a sequence given the previous characters. You will build character-level n-gram language models. You will generate text from models you create, as well as use perplexity to measure the fit of various language models on test data related and unrelated to the training data. The homework is designed to implement on your own computer or a server of your choosing, but you can also use class CRCD resources if you like. Just note that you’ll have to submit a Python script, not a Jupyter notebook."
  },
  {
    "objectID": "hw1.html#learning-objectives",
    "href": "hw1.html#learning-objectives",
    "title": "Homework 1: Character-level language modeling",
    "section": "Learning objectives",
    "text": "Learning objectives\nAfter completing this assignment, students will be able to:\n\nUnderstand how to compute n-gram language model probabilities using maximum likelihood estimation.\nUse n-gram language models to probabilistically generate texts and implement generation using n-gram probabilities.\nGain intuition on how perplexity will estimate language model performance on unseen texts."
  },
  {
    "objectID": "hw1.html#extract-character-n-grams",
    "href": "hw1.html#extract-character-n-grams",
    "title": "Homework 1: Character-level language modeling",
    "section": "1. Extract character n-grams",
    "text": "1. Extract character n-grams\nFirst, fill out the ngrams(c, text) function that produces a list of all n-grams of that use c elements of context from the input text. Each n-gram should consist of a 2-element tuple (context, char), where the context is itself a c-length string comprised of the c characters preceding the current character. If c == 1, then produce bigrams, if c == 2, trigrams. The sentence should be padded with c ~ characters at the beginning (we’ve provided you with start_pad(c) for this purpose). If c == 0, all contexts should be empty strings. You may assume that c ≥ 0. You are allowed to use any resources or packages to extract the character ngrams from text, such as scikit-learn or NLTK. Here is some example output from such a function:\n&gt;&gt;&gt; ngrams(1, 'abc')\n[('~', 'a'), ('a', 'b'), ('b', 'c')]\n\n&gt;&gt;&gt; ngrams(2, 'abc')\n[('~~', 'a'), ('~a', 'b'), ('ab', 'c')]\nWe’ve also given you the function create_ngram_model(model_class, path, c) that will create and return an n-gram model trained on the entire file path provided."
  },
  {
    "objectID": "hw1.html#build-n-gram-language-models",
    "href": "hw1.html#build-n-gram-language-models",
    "title": "Homework 1: Character-level language modeling",
    "section": "2. Build n-gram language models",
    "text": "2. Build n-gram language models\nIn this section, you will build a simple n-gram language model that can be used to generate random text resembling a source document. This model does not need any smoothing.\nIn the NgramModel class, write an initialization method __init__(self, c) which stores the context length c of the model and initializes any necessary internal variables. Then write a method get_vocab(self) that returns the vocab (this is the set of all characters used by this model).\nWrite a method update(self, text) which computes the n-grams for the input sentence and updates the internal counts (storing counts is recommended instead of storing probabilities). Also, write a method prob(self, context, char) which accepts a c-length string representing a context and a character, and returns the probability of that character occurring, given the preceding context. Characters that have never been seen before in a certain context would be assigned a 0 probability. If you encounter a novel context (one that has never been seen before in training data), the probability of any given character should be \\(1/V\\) where \\(V\\) is the size of the vocabulary. See Chapter 3 of the Jurafsky and Martin textbook and Equation 3.12 for calculating probabilities based on observed counts. You may not use any package to directly train/compute language model probabilities; that portion of the program should be from scratch.\n &gt;&gt;&gt; m = NgramModel(1)\n &gt;&gt;&gt; m.update('abab')\n &gt;&gt;&gt; m.get_vocab()\n {'a', 'b'}\n &gt;&gt;&gt; m.update('abcd')\n &gt;&gt;&gt; m.get_vocab()\n {'a', 'b', 'c', 'd'}\n &gt;&gt;&gt; m.prob('a', 'b')\n 1.0\n &gt;&gt;&gt; m.prob('~', 'c')\n 0.0\n &gt;&gt;&gt; m.prob('b', 'c')\n 0.5\nWrite a method random_char(self, context) which returns a random character according to the probability distribution determined by the given context. Just like the prob function, in a novel context assign a probability of any given character \\(1/V\\), where \\(V\\) is the size of the vocabulary.\n\nHere is some example output. Even with setting the random seed, your output does not need to perfectly match the example output as there are multiple functions that can perform this task.\n     &gt;&gt;&gt; m = NgramModel(0)\n     &gt;&gt;&gt; m.update('abab')\n     &gt;&gt;&gt; m.update('abcd')\n     &gt;&gt;&gt; random.seed(1)\n     &gt;&gt;&gt; [m.random_char('') for i in range(10)]\n     ['a', 'c', 'c', 'a', 'b', 'b', 'b', 'c', 'a', 'a']\nIn the NgramModel class, write a method random_text(self, length) which returns a string of characters chosen at random using the random_char(self, context) method. Your starting context should always be c ~ characters, and the context should be updated as characters are generated. If c == 0, your context should always be the empty string. You should continue generating characters until you’ve produced the specified number of random characters, then return the full string.\nHere is some example output. Even with setting the random seed, your output does not need to perfectly match the example output as there are multiple functions that can perform this task.\n     &gt;&gt;&gt; m = NgramModel(1)\n     &gt;&gt;&gt; m.update('abab')\n     &gt;&gt;&gt; m.update('abcd')\n     &gt;&gt;&gt; random.seed(1)\n     &gt;&gt;&gt; m.random_text(10)\n     abcdbabcda"
  },
  {
    "objectID": "hw1.html#generating-shakespeare-with-character-level-n-gram-language-models",
    "href": "hw1.html#generating-shakespeare-with-character-level-n-gram-language-models",
    "title": "Homework 1: Character-level language modeling",
    "section": "3. Generating Shakespeare with character-level n-gram language models",
    "text": "3. Generating Shakespeare with character-level n-gram language models\nNow you can train a language model using the training corpus of Shakespeare. Afterward, try generating some Shakespeare with different order character n-gram models. Do this for at least trigrams, 4-grams and 7-grams, but feel free to explore other values of n as well. The following commands should produce text using different values of n:\n&gt;&gt;&gt; m = create_ngram_model(NgramModel, 'shakespeare_input.txt', 2)\n&gt;&gt;&gt; m.random_text(250)\n\n&gt;&gt;&gt; m = create_ngram_model(NgramModel, 'shakespeare_input.txt', 3)\n&gt;&gt;&gt; m.random_text(250)\n\n&gt;&gt;&gt; m = create_ngram_model(NgramModel, 'shakespeare_input.txt', 4)\n&gt;&gt;&gt; m.random_text(250)\n\n&gt;&gt;&gt; m = create_ngram_model(NgramModel, 'shakespeare_input.txt', 7)\n&gt;&gt;&gt; m.random_text(250)\nYou may make any additional assumptions and design decisions, but state them in your report (see below). For example, some design choices that could be made are how you want to handle uppercase and lowercase letters or how you want to handle digits. The choice made is up to you, we only require that you detail these decisions in your report and consider any implications of them in your results. There is no wrong choice here, and these decisions are typically made by NLP researchers when pre-processing data."
  },
  {
    "objectID": "hw1.html#calculate-perplexity-of-test-documents",
    "href": "hw1.html#calculate-perplexity-of-test-documents",
    "title": "Homework 1: Character-level language modeling",
    "section": "4. Calculate perplexity of test documents",
    "text": "4. Calculate perplexity of test documents\nUsing the perplexity method, calculate the perplexity of each test document. For each file in the test data (nytimes_article.txt and shakespeare_sonnets.txt), calculate the perplexity for each non-blank line and the average across all lines in the document. Do this for trigram, 4-gram and 7-gram character-level language models trained on Shakespeare plays (shakespeare_input.txt)."
  },
  {
    "objectID": "hw1.html#deliverables",
    "href": "hw1.html#deliverables",
    "title": "Homework 1: Character-level language modeling",
    "section": "Deliverables",
    "text": "Deliverables\nIn your report, include:\n\nA description of how you wrote your program, including all assumptions and design decisions\nReport examples of generated text for trigram, 4-gram and 7-gram models. Discuss these results. What do you notice about the short passages you’ve generated from n-gram models with different n? Are they as good as 1000 monkeys working at 1000 typewriters? Are there patterns in what models generate first?\nPerplexity values for trigram, 4-gram, and 7-gram character-level language models on each test file (New York Times article and Shakespeare sonnets).\nWhat does your perplexity indicate across different test documents? What does a comparison of different n in the n-grams in terms of perplexity tell you? Which performs best? Why do you think your models performed the way they did?"
  },
  {
    "objectID": "hw1.html#acknowledgments",
    "href": "hw1.html#acknowledgments",
    "title": "Homework 1: Character-level language modeling",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis assignment is based on a homework assignment by Prof. Diane Litman and Prof. Mark Yatskar. Shakespeare data is from Andrej Karpathy."
  },
  {
    "objectID": "hw3.html",
    "href": "hw3.html",
    "title": "Homework 3: Large language model (LLM) prompting and instruction tuning",
    "section": "",
    "text": "Due 2025-11-07, 11:59pm. Instructions last updated 2025-11-03."
  },
  {
    "objectID": "hw3.html#learning-objectives",
    "href": "hw3.html#learning-objectives",
    "title": "Homework 3: Large language model (LLM) prompting and instruction tuning",
    "section": "Learning objectives",
    "text": "Learning objectives\nAfter completing this assignment, students will be able to:\n\nPrompt LLMs programmatically with templates (parameterized)\nPrompt LLMs with zero-shot and few-shot prompting\nEngineer and test different prompts\nImplement instruction tuning of LLMs using Hugging Face tools\nEvaluate instruction-tuned LLMs against base models"
  },
  {
    "objectID": "hw3.html#overview",
    "href": "hw3.html#overview",
    "title": "Homework 3: Large language model (LLM) prompting and instruction tuning",
    "section": "Overview",
    "text": "Overview\nIn this assignment, you will explore different prompting techniques LLMs (Part 1). You will also instruction-tune an LLM (Part 2). Both of these parts will involve filling in template Jupyter notebooks onPitt’s CRCD through the class JupyterHub. You will need GPU resources for Part 2. You can use the Nvidia L4 GPU on the CRCD JupyterHub (teach cluster) like we use in class, or CRCD’s Open OnDemand Jupyter service. Use the class conda environment to load all necessary packages.\nTo get started, click on the class nbgitpuller link."
  },
  {
    "objectID": "hw3.html#part-1-llm-prompting",
    "href": "hw3.html#part-1-llm-prompting",
    "title": "Homework 3: Large language model (LLM) prompting and instruction tuning",
    "section": "Part 1: LLM prompting",
    "text": "Part 1: LLM prompting\nPlease fill out the template notebook, hw3_template_part1.ipynb, from the class CRCD JupyterHub."
  },
  {
    "objectID": "hw3.html#part-2-instruction-tuning-llms",
    "href": "hw3.html#part-2-instruction-tuning-llms",
    "title": "Homework 3: Large language model (LLM) prompting and instruction tuning",
    "section": "Part 2: Instruction tuning LLMs",
    "text": "Part 2: Instruction tuning LLMs\nFill out hw3_template_part2.ipynb from the class CRCD JupyterHub."
  },
  {
    "objectID": "hw3.html#deliverables",
    "href": "hw3.html#deliverables",
    "title": "Homework 3: Large language model (LLM) prompting and instruction tuning",
    "section": "Deliverables",
    "text": "Deliverables\n\nYour code: the Jupyter notebooks you modified from the templates for parts 1 and 2. Submit:\n\nyour .ipynb files for parts 1 and 2\na .html export of your notebooks from parts 1 and 2. To get a .html version, click File &gt; Save and Export Notebook As… &gt; HTML from within JupyterLab.\n\nA PDF report with answers to questions provided in the template notebooks. Please name your report hw3_{your pitt email id}.pdf. No need to include @pitt.edu, just use the email ID before that part. For example: report_mmyoder_hw3.pdf. Please make only one PDF report, containing answers to part 1 and part 2. Make sure to include the following information:\n\nanswers to all the numbered questions in the templates\nany additional resources, references, or web pages you’ve consulted\nany person with whom you’ve discussed the assignment and describe the nature of your discussions\nany generative AI tool used, and how it was used\nany unresolved issues or problems\n\n\nPlease submit all of this material on Canvas. We will grade your report and may look over your code."
  },
  {
    "objectID": "hw3.html#acknowledgments",
    "href": "hw3.html#acknowledgments",
    "title": "Homework 3: Large language model (LLM) prompting and instruction tuning",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nPart 1 of this assignment is based on a homework assignment designed by Mark Yatskar and provided by Lorraine Li."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS 2731 Introduction to Natural Language Processing",
    "section": "",
    "text": "School of Computing and Information, University of Pittsburgh\nFall 2025"
  },
  {
    "objectID": "index.html#participation-grade",
    "href": "index.html#participation-grade",
    "title": "CS 2731 Introduction to Natural Language Processing",
    "section": "Participation grade",
    "text": "Participation grade\nIn-class, collaborative activities are better learning experiences when students come to class and participate. To encourage participation, there is a participation grade worth 10% of the total course grade. The majority of that grade comes from attendance, which will be taken via Top Hat on randomly selected class sessions. The rest of the grade will be assigned based on whether a student asked questions in class or otherwise (such as during office hours), or partipated in in-class activites. If you did any of this basic engagement, full credit will be awarded."
  },
  {
    "objectID": "index.html#grading-scale",
    "href": "index.html#grading-scale",
    "title": "CS 2731 Introduction to Natural Language Processing",
    "section": "Grading scale",
    "text": "Grading scale\n\n\n\nRange\nLetter grade\n\n\n\n\n92.5 – 100%\nA\n\n\n90.0 – &lt;92.5%\nA-\n\n\n87.5 – &lt;90.0%\nB+\n\n\n82.5 – &lt;87.5%\nB\n\n\n80.0 – &lt;82.5%\nB-\n\n\n77.5 – &lt;80.0%\nC+\n\n\n72.5 – &lt;77.5%\nC\n\n\n70.0 – &lt;72.5%\nC-\n\n\n67.5 – &lt;70.0%\nD+\n\n\n62.5 – &lt;67.5%\nD\n\n\n60.0 – &lt;62.5%\nD-\n\n\n&lt; 60%\nF\n\n\n\nFeel free to contact the instructor or schedule an office hours appointment to talk about any issues you might have with your grade."
  },
  {
    "objectID": "index.html#late-work-policy",
    "href": "index.html#late-work-policy",
    "title": "CS 2731 Introduction to Natural Language Processing",
    "section": "Late work policy",
    "text": "Late work policy\nStudents are granted 5 total late days across all homework assignments and quizzes without penalty. After those five late days, you will be penalized 10% for each day that your submission is late up to a maximum of 40%. Group project work will be penalized 10% for each day late up to a maximum of 40%. No late work will be accepted for the project report."
  },
  {
    "objectID": "index.html#assignment-resubmission-policy",
    "href": "index.html#assignment-resubmission-policy",
    "title": "CS 2731 Introduction to Natural Language Processing",
    "section": "Assignment resubmission policy",
    "text": "Assignment resubmission policy\nIf you are unsatisfied with your grade on an assignment and wish to resubmit work, talk with the instructor. Resubmissions are handled case by case, but are generally accepted in cases where parts of the assignment are missing (sections of the rubric are 0). Updated or added text in resubmitted reports must be highlighted in yellow. Resubmissions are subject to an automatic 10% deduction and must be submitted by 11:59pm on the last day of class. Only 1 resubmission per homework assignment will be accepted."
  },
  {
    "objectID": "index.html#academic-integrity-policy",
    "href": "index.html#academic-integrity-policy",
    "title": "CS 2731 Introduction to Natural Language Processing",
    "section": "Academic integrity policy",
    "text": "Academic integrity policy\nStudents in this course will be expected to comply with the University of Pittsburgh’s Policy on Academic Integrity. Any student suspected of violating this obligation for any reason during the semester will be required to participate in the procedural process, initiated at the instructor level, as outlined in the University Guidelines on Academic Integrity. To learn more about Academic Integrity, visit the Academic Integrity Guide for an overview of the topic. For hands-on practice, complete the Academic Integrity Modules.\n\nGenerative AI policy\nYou are allowed to use generative AI programs (ChatGPT, etc.) as a student in this course in limited circumstances. Since much of this course is about developing such tools in NLP, using currently available tools can expose you to the current capabilities and limitations of such systems.\nHowever, your ethical responsibilities as a student remain the same. You must follow the University of Pittsburgh’s Policy on Academic Integrity. Here are some principles to keep in mind that can help you determine whether or not a specific use of generative AI is acceptable in this course (for all forms of generation: writing, code, images or other forms). Please ask the instructor if you are not sure about a specific use. You will not be blamed or retaliated against for asking.\n\nUse as an aid, not for a finished product. LLMs could be used in this course to generate ideas, draft bibliographies, study guides, or for revising existing writing. Use for drafting entire homework or project reports is not acceptable, even if students revise this draft, since being able to communicate NLP procedures and research is a learning objective. Also keep in mind that language models have no notion of reality and will hallucinate facts and citations.\nCite its use. The University of Pittsburgh’s academic integrity policy applies to all uncited or improperly cited use of content, whether that work is created by human beings alone or in collaboration with a generative AI. If you use a generative AI tool to develop content for an assignment, you are required to cite the tool’s contribution to your work. In practice, cutting and pasting content from any source without citation is plagiarism. Likewise, paraphrasing content from a generative AI without citation is plagiarism. Similarly, using any generative AI tool without appropriate acknowledgement will be treated as plagiarism. See the APA guidelines on how to cite ChatGPT. Citing your use of LLMs will also inform teaching staff on how such tools are being used in education for developing better future policies.\nYou are responsible for the work you turn in. As we will discuss in this course, LLMs and other generative AI systems can and do generate biased, socially problematic language and assert unfounded claims. Ultimately the text you submit will be treated as reflecting your own work, and you are responsible for it.\n\nAdapted from faculty in the Carnegie Mellon University Heinz College of Information Systems and Public Policy, with guidance from the Carnegie Mellon University Eberly Center for Teaching Excellence."
  },
  {
    "objectID": "index.html#disability-rights",
    "href": "index.html#disability-rights",
    "title": "CS 2731 Introduction to Natural Language Processing",
    "section": "Disability rights",
    "text": "Disability rights\nThe teaching staff of this course view disabilities as deficits not in disabled people but in the institutions and societies that are structured to disadvantage disabled people. If you have a disability (visible or invisible), please let us know as soon as possible. You don’t need to tell us the nature of the disability. You are encouraged to work with Disability Resources and Services (DRS), 140 William Pitt Union, (412) 648-7890, drsrecep@pitt.edu, (412) 228-5347 for P3 ASL users, as early as possible in the term. DRS will work with you to determine reasonable accommodations for this course. This might include lecture materials that are usable by people with visual disabilities, sign language interpretation, captioning, flexible due dates, etc.\nAdapted from policies by David Mortensen and Lori Levin at Carnegie Mellon University."
  },
  {
    "objectID": "index.html#religious-observances",
    "href": "index.html#religious-observances",
    "title": "CS 2731 Introduction to Natural Language Processing",
    "section": "Religious Observances",
    "text": "Religious Observances\nThe observance of religious holidays (activities observed by a religious group of which a student is a member) and cultural practices are an important reflection of diversity. As your instructor, I am committed to providing equivalent educational opportunities to students of all belief systems. At the beginning of the semester, you should review the course requirements to identify foreseeable conflicts with assignments or other required attendance. Please contact me as early as possible to allow time for us to discuss and make fair and reasonable adjustments to the schedule and/or tasks."
  },
  {
    "objectID": "index.html#statement-on-scholarly-discourse",
    "href": "index.html#statement-on-scholarly-discourse",
    "title": "CS 2731 Introduction to Natural Language Processing",
    "section": "Statement on scholarly discourse",
    "text": "Statement on scholarly discourse\nIn this course we will be discussing some complex issues on which all of us have strong feelings and, in many cases, unfounded attitudes. It is essential that we approach this endeavor with our minds open to evidence that may conflict with our presuppositions. Moreover, it is vital that we treat each other’s opinions and comments with courtesy even when they diverge and conflict with our own. We must avoid personal attacks and the use of ad hominem arguments to invalidate each other’s positions. Instead, we must develop a culture of civil argumentation, wherein all positions have the right to be defended and argued against in intellectually reasoned ways.\nAdapted from a California State University course: Race, Racism and Critical Thinking."
  },
  {
    "objectID": "index.html#student-wellness",
    "href": "index.html#student-wellness",
    "title": "CS 2731 Introduction to Natural Language Processing",
    "section": "Student wellness",
    "text": "Student wellness\nCollege can be an exciting and challenging time for students. Taking time to maintain your well-being and seek appropriate support can help you achieve your goals and lead a fulfilling life. It can be helpful to remember that we all benefit from assistance and guidance at times, and there are many resources available to support your well-being while you are at Pitt. You are encouraged to visit Thrive@Pitt to learn more about well-being and the many campus resources available to help you thrive.\nIf you or anyone you know experiences overwhelming academic stress, persistent difficult feelings and/or challenging life events, you are strongly encouraged to seek support. In addition to reaching out to friends and loved ones, consider connecting with a faculty member you trust for assistance connecting to helpful resources.\nThe University Counseling Center is also here for you. You can call 412-648-7930 at any time to connect with a clinician. If you or someone you know is feeling suicidal, please call the University Counseling Center at any time at 412-648-7930. You can also contact Resolve Crisis Network at 888-796-8226."
  },
  {
    "objectID": "index.html#equity-and-inclusion",
    "href": "index.html#equity-and-inclusion",
    "title": "CS 2731 Introduction to Natural Language Processing",
    "section": "Equity and inclusion",
    "text": "Equity and inclusion\nThe University of Pittsburgh does not tolerate any form of discrimination, harassment, or retaliation based on disability, race, color, religion, national origin, ancestry, genetic information, marital status, familial status, sex, age, sexual orientation, veteran status or gender identity or other factors as stated in the University’s Title IX policy. The University is committed to taking prompt action to end a hostile environment that interferes with the University’s mission. For more information about policies, procedures, and practices, visit the Civil Rights & Title IX Compliance web page.\nI ask that everyone in the class strive to help ensure that other members of this class can learn in a supportive and respectful environment. If there are instances of the aforementioned issues, please contact the Title IX Coordinator, by calling 412-648-7860, or emailing titleixcoordinator@pitt.edu. Reports can also be filed online. You may also choose to report this to a faculty/staff member; they are required to communicate this to the University’s Office of Diversity and Inclusion. If you wish to maintain complete confidentiality, you may also contact the University Counseling Center (412-648-7930)."
  }
]